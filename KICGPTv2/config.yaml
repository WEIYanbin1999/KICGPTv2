# KICGPTv2 Configuration File
# This file contains all hyperparameters and settings as disclosed in the manuscript and Appendix

# ============================================================================
# LLM API Configuration (Reviewer Requirement)
# ============================================================================
llm:
  model_name: "Qwen/Qwen2.5-72B-Instruct"  # Qwen-2.5-72B can be accessed via API
  api_base: "https://api.pumpkinaigc.online/v1"  # API endpoint
  api_key: ""  # Set via --api_key argument or environment variable
  temperature: 0
  max_tokens: 9600  # Maximum output tokens for LLM responses
  max_llm_input_tokens: 3750  # Maximum input context length
  top_p: 1
  frequency_penalty: 0
  presence_penalty: 0

# ============================================================================
# Dataset Configuration
# ============================================================================
datasets:
  base_path: "./datasets"  # Base directory for all datasets
  
  # Supported datasets
  fb15k-237:
    entity_file: "entity2text.txt"
    relation_file: "relation2text.txt"
    train_file: "get_neighbor/train2id.txt"
    valid_file: "get_neighbor/valid2id.txt"
    test_file: "get_neighbor/test2id.txt"
    entity_contexts: "entity_contexts.txt"  # Generated by preprocessing
    relation_alignment: "alignment/alignment_clean.txt"  # Generated by preprocessing
    
  wn18rr:
    entity_file: "entity2text.txt"
    relation_file: "relation2text.txt"
    train_file: "get_neighbor/train2id.txt"
    valid_file: "get_neighbor/valid2id.txt"
    test_file: "get_neighbor/test2id.txt"
    entity_contexts: "entity_contexts.txt"
    relation_alignment: "alignment/alignment_clean.txt"
    
  fb13:
    entity_file: "entity2text.txt"
    relation_file: "relation2text.txt"
    train_file: "get_neighbor/train2id.txt"
    test_file: "test_classification.txt"
    entity_contexts: "entity_contexts.txt"
    relation_alignment: "alignment/alignment_clean.txt"
    
  umls:
    entity_file: "entity2text.txt"
    relation_file: "relation2text.txt"
    train_file: "get_neighbor/train2id.txt"
    test_file: "test_classification.txt"
    entity_contexts: "entity_contexts.txt"
    relation_alignment: "alignment/alignment_clean.txt"
    
  wikidata5m:
    entity_file: "entity2text.txt"
    relation_file: "relation2text.txt"
    train_file: "get_neighbor/train2id.txt"
    valid_file: "get_neighbor/valid2id.txt"
    test_file: "get_neighbor/test2id.txt"
    entity_contexts: "entity_contexts.txt"
    relation_alignment: "alignment/alignment_clean.txt"

# ============================================================================
# Task-Specific Hyperparameters (As per Appendix B Sensitivity Analysis)
# ============================================================================

# Link Prediction Task
# Appendix Table: Best performance at m=30 or m=50, δ=16
link_prediction:
  candidate_num_m: 30  # Number of top candidates for re-ranking (m)
  demonstration_num_delta: 16  # Number of demonstrations (δ)
  similarity_beta: 0.33  # Similarity threshold for retrieval-augmented reconstruction (β)
  demon_per_step: 8  # Demonstrations added per step
  eff_demon_step: 10  # Effective demonstration steps
  max_demon_step: 10  # Maximum demonstration steps
  use_entity_context: true  # Enable entity context-extraction
  use_relation_alignment: true  # Enable relation self-alignment
  use_reconstruction: true  # Enable retrieval-augmented reconstruction
  
  # Alternative configurations (for ablation studies)
  alternatives:
    m10_delta16:
      candidate_num_m: 10
      demonstration_num_delta: 16
    m30_delta8:
      candidate_num_m: 30
      demonstration_num_delta: 8
    m30_delta32:
      candidate_num_m: 30
      demonstration_num_delta: 32
    m50_delta16:
      candidate_num_m: 50
      demonstration_num_delta: 16

# Relation Prediction Task  
# Appendix Table: Best performance at m=10, δ=8 or δ=16
relation_prediction:
  candidate_num_m: 10  # Number of top candidates for re-ranking (m)
  demonstration_num_delta: 8  # Number of demonstrations (δ=8 best as per Appendix)
  similarity_beta: 0.33  # Similarity threshold (β)
  demon_per_step: 8
  eff_demon_step: 8  # Changed from 10 to 8 to match Appendix B best performance
  max_demon_step: 10
  use_entity_context: true
  use_relation_alignment: true
  use_reconstruction: true
  
  # Alternative configurations
  alternatives:
    m10_delta16:
      demonstration_num_delta: 16
    m10_delta32:
      demonstration_num_delta: 32

# Triple Classification Task
# Appendix Table: Best performance at δ=32
triple_classification:
  candidate_num_m: 2  # Binary classification: {Yes, No}
  demonstration_num_delta: 32  # Number of demonstrations (δ)
  demon_per_step: 8
  eff_demon_step: 32
  max_demon_step: 32
  use_entity_context: true
  use_relation_alignment: true
  # Note: No reconstruction needed for binary classification
  
  # Alternative configurations
  alternatives:
    delta8:
      demonstration_num_delta: 8
    delta16:
      demonstration_num_delta: 16

# ============================================================================
# Preprocessing Configuration
# ============================================================================
preprocessing:
  # Entity Context-extraction
  entity_context_extraction:
    enabled: true
    prompt_file: "./prompts/entity_context.json"
    prompt_name: "chat"
    max_tokens: 300
    num_process: 1  # Parallel processes for extraction
    
  # Relation Self-alignment
  relation_alignment:
    enabled: true
    prompt_file: "./prompts/text_alignment.json"
    prompt_name: "chat"
    max_tokens: 300
    num_process: 1

# ============================================================================
# Prompt Templates
# ============================================================================
prompts:
  link_prediction: "./prompts/link_prediction.json"
  relation_prediction: "./prompts/relation_prediction.json"
  triple_classification: "./prompts/triple_classification.json"
  entity_context: "./prompts/entity_context.json"
  text_alignment: "./prompts/text_alignment.json"

# ============================================================================
# Output Configuration
# ============================================================================
outputs:
  base_path: "./outputs"
  save_chat_logs: true
  overwrite: false  # Whether to overwrite existing output files

# ============================================================================
# Execution Configuration
# ============================================================================
execution:
  debug: false  # Enable debug mode (manual input)
  debug_online: false  # Test with subset of data
  num_process: 1  # Number of parallel processes
  device: 0  # GPU device ID (for base KGC model if needed)

# ============================================================================
# Base KGC Model Configuration
# ============================================================================
base_model:
  # The base KGC model provides preliminary ordering (L_pre)
  # Examples: TransE, RotatE, KG-BERT, LMKE, etc.
  model_type: "KG-BERT"  # or "LMKE", "TransE", etc.
  model_path: null  # Path to pretrained base model (if applicable)
  
# ============================================================================
# Notes from Manuscript
# ============================================================================
# 1. KICGPTv2 Framework Stages (Algorithm \ref{entire_alg}):
#    - Entity Context-extraction: Extract (t_e, τ_e) for all entities
#    - Relation Self-alignment: Refine relations ř using LLM
#    - Preliminary Ordering: Base KGC model scores candidates → L_pre
#    - Knowledge-prompting Re-ranking: LLM reranks top-m with demonstrations → L_LLM
#    - Retrieval-augmented Reconstruction: Match LLM output to candidates → L_KICGPTv2
#
# 2. Hyperparameter Sensitivity (Appendix B):
#    - Link Prediction: m ∈ {10,30,50}, δ ∈ {8,16,32}, best: m=30, δ=16
#    - Relation Prediction: m=10 (sufficient), δ ∈ {8,16,32}, best: δ=8 or 16
#    - Triple Classification: δ=32 (more demonstrations help binary decision)
#    - β=33 (0.33) is robust across all tasks
#
# 3. Model Selection:
#    - LLM: Qwen-2.5-72B (accessible via API)
#    - Base KGC: KG-BERT (semantic-based) or LMKE (for long-tail)
#
# 4. Training Overhead:
#    - KICGPTv2 is training-free for LLM (no finetuning required)
#    - Only base KGC model needs training on KG structure
